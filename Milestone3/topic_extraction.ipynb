{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import LdaMulticore\n",
    "import spacy\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os, bz2, time\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(stemmer, text):\n",
    "#     '''lemmatize and stem the text to get key tokens'''\n",
    "#     return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# # Tokenize and lemmatize\n",
    "# def preprocess(text, names):\n",
    "#     '''preprocess the quotation list and extract tokens to dictionary'''\n",
    "#     result=[]\n",
    "#     for token in gensim.utils.simple_preprocess(text):\n",
    "#         if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in names:\n",
    "#             result.append(lemmatize_stemming(SnowballStemmer(\"english\"), token))\n",
    "#     return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all quotations data of US\n",
    "USA_DATA = '../data/quotes_mentions_USA_compact.json.bz2'\n",
    "df = pd.read_json(USA_DATA, lines=True, compression='bz2' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>phase</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions_qids</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30190</th>\n",
       "      <td>2015-01-01-004934</td>\n",
       "      <td>David Vitter pursues similar line of attack to...</td>\n",
       "      <td>Chuck Kleckley</td>\n",
       "      <td>Q5115563</td>\n",
       "      <td>2015-01-01 00:16:22</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[David Vitter, Chuck Kleckley]</td>\n",
       "      <td>[Q519780, Q5115563]</td>\n",
       "      <td>[bayoubuzz.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27435</th>\n",
       "      <td>2015-01-01-004937</td>\n",
       "      <td>David Vitter pursues similar line of attack to...</td>\n",
       "      <td>Chuck Kleckley</td>\n",
       "      <td>Q5115563</td>\n",
       "      <td>2015-01-01 00:46:28</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[David Vitter, Steve Scalise]</td>\n",
       "      <td>[Q519780, Q1857141]</td>\n",
       "      <td>[bayoubuzz.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>2015-01-01-020316</td>\n",
       "      <td>I've joked around a little bit with Rahm Emanu...</td>\n",
       "      <td>Kirk Caldwell</td>\n",
       "      <td>Q6415403</td>\n",
       "      <td>2015-01-01 01:00:02</td>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "      <td>[Rahm Emanuel]</td>\n",
       "      <td>[Q298443]</td>\n",
       "      <td>[gantdaily.com, cnn.com, kwch.com, wsbt.com, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41217</th>\n",
       "      <td>2015-01-01-037000</td>\n",
       "      <td>We have expressed our disagreement with the fe...</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>Q207</td>\n",
       "      <td>2015-01-01 01:00:02</td>\n",
       "      <td>5</td>\n",
       "      <td>E</td>\n",
       "      <td>[Bill Clinton]</td>\n",
       "      <td>[Q1124]</td>\n",
       "      <td>[www.wimsradio.com, 6abc.com, abc7.com, abc7ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43062</th>\n",
       "      <td>2015-01-01-027335</td>\n",
       "      <td>Thank you for your support. Jeb Bush</td>\n",
       "      <td>George W. Bush</td>\n",
       "      <td>Q207</td>\n",
       "      <td>2015-01-01 01:00:02</td>\n",
       "      <td>5</td>\n",
       "      <td>E</td>\n",
       "      <td>[George W. Bush]</td>\n",
       "      <td>[Q207]</td>\n",
       "      <td>[www.wimsradio.com, 6abc.com, abc7.com, abc7ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436341</th>\n",
       "      <td>2020-04-16-008907</td>\n",
       "      <td>Crazy `Nancy Pelosi, you are a weak person. Yo...</td>\n",
       "      <td>President Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2020-04-16 20:36:52</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Nancy Pelosi]</td>\n",
       "      <td>[Q170581]</td>\n",
       "      <td>[www.sfchronicle.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431915</th>\n",
       "      <td>2020-04-16-038512</td>\n",
       "      <td>President Trump has directed our team to devel...</td>\n",
       "      <td>Mike Pence</td>\n",
       "      <td>Q24313</td>\n",
       "      <td>2020-04-16 20:47:30</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[www.wtkr.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433067</th>\n",
       "      <td>2020-04-16-017107</td>\n",
       "      <td>I don't have the Jared Kushners to call and sa...</td>\n",
       "      <td>Joe Manchin</td>\n",
       "      <td>Q538868</td>\n",
       "      <td>2020-04-16 21:05:00</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>[Jared Kushner]</td>\n",
       "      <td>[Q13628723]</td>\n",
       "      <td>[khn.org]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425295</th>\n",
       "      <td>2020-04-16-012002</td>\n",
       "      <td>focused on Sleepy Joe Biden,</td>\n",
       "      <td>Linda McMahon</td>\n",
       "      <td>Q233905</td>\n",
       "      <td>2020-04-16 21:05:39</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Joe Biden]</td>\n",
       "      <td>[Q6279]</td>\n",
       "      <td>[www.vice.com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436212</th>\n",
       "      <td>2020-04-16-038506</td>\n",
       "      <td>President Trump basically said, `Yeah, that's ...</td>\n",
       "      <td>Pat Toomey</td>\n",
       "      <td>Q971308</td>\n",
       "      <td>2020-04-16 21:14:42</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[www.the-leader.com, www.doverpost.com]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>450436 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  quoteID                                          quotation  \\\n",
       "30190   2015-01-01-004934  David Vitter pursues similar line of attack to...   \n",
       "27435   2015-01-01-004937  David Vitter pursues similar line of attack to...   \n",
       "32557   2015-01-01-020316  I've joked around a little bit with Rahm Emanu...   \n",
       "41217   2015-01-01-037000  We have expressed our disagreement with the fe...   \n",
       "43062   2015-01-01-027335               Thank you for your support. Jeb Bush   \n",
       "...                   ...                                                ...   \n",
       "436341  2020-04-16-008907  Crazy `Nancy Pelosi, you are a weak person. Yo...   \n",
       "431915  2020-04-16-038512  President Trump has directed our team to devel...   \n",
       "433067  2020-04-16-017107  I don't have the Jared Kushners to call and sa...   \n",
       "425295  2020-04-16-012002                       focused on Sleepy Joe Biden,   \n",
       "436212  2020-04-16-038506  President Trump basically said, `Yeah, that's ...   \n",
       "\n",
       "                speaker      qids                date  numOccurrences phase  \\\n",
       "30190    Chuck Kleckley  Q5115563 2015-01-01 00:16:22               1     E   \n",
       "27435    Chuck Kleckley  Q5115563 2015-01-01 00:46:28               1     E   \n",
       "32557     Kirk Caldwell  Q6415403 2015-01-01 01:00:02               8     E   \n",
       "41217    George W. Bush      Q207 2015-01-01 01:00:02               5     E   \n",
       "43062    George W. Bush      Q207 2015-01-01 01:00:02               5     E   \n",
       "...                 ...       ...                 ...             ...   ...   \n",
       "436341  President Trump    Q22686 2020-04-16 20:36:52               1     E   \n",
       "431915       Mike Pence    Q24313 2020-04-16 20:47:30               1     E   \n",
       "433067      Joe Manchin   Q538868 2020-04-16 21:05:00               4     E   \n",
       "425295    Linda McMahon   Q233905 2020-04-16 21:05:39               1     E   \n",
       "436212       Pat Toomey   Q971308 2020-04-16 21:14:42               2     E   \n",
       "\n",
       "                              mentions        mentions_qids  \\\n",
       "30190   [David Vitter, Chuck Kleckley]  [Q519780, Q5115563]   \n",
       "27435    [David Vitter, Steve Scalise]  [Q519780, Q1857141]   \n",
       "32557                   [Rahm Emanuel]            [Q298443]   \n",
       "41217                   [Bill Clinton]              [Q1124]   \n",
       "43062                 [George W. Bush]               [Q207]   \n",
       "...                                ...                  ...   \n",
       "436341                  [Nancy Pelosi]            [Q170581]   \n",
       "431915                  [Donald Trump]             [Q22686]   \n",
       "433067                 [Jared Kushner]          [Q13628723]   \n",
       "425295                     [Joe Biden]              [Q6279]   \n",
       "436212                  [Donald Trump]             [Q22686]   \n",
       "\n",
       "                                                     urls  \n",
       "30190                                     [bayoubuzz.com]  \n",
       "27435                                     [bayoubuzz.com]  \n",
       "32557   [gantdaily.com, cnn.com, kwch.com, wsbt.com, n...  \n",
       "41217   [www.wimsradio.com, 6abc.com, abc7.com, abc7ch...  \n",
       "43062   [www.wimsradio.com, 6abc.com, abc7.com, abc7ch...  \n",
       "...                                                   ...  \n",
       "436341                              [www.sfchronicle.com]  \n",
       "431915                                     [www.wtkr.com]  \n",
       "433067                                          [khn.org]  \n",
       "425295                                     [www.vice.com]  \n",
       "436212            [www.the-leader.com, www.doverpost.com]  \n",
       "\n",
       "[450436 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by='date', ascending=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars 10079439 for year 2015\n",
      "number of chars 13294813 for year 2016\n",
      "number of chars 21151589 for year 2017\n",
      "number of chars 20394421 for year 2018\n",
      "number of chars 17056268 for year 2019\n",
      "number of chars 5227609 for year 2020\n"
     ]
    }
   ],
   "source": [
    "# convert quotations to corpus by year\n",
    "corpus_root = \"./\"\n",
    "for year in range(2015, 2021):\n",
    "    file_path = '../data/quotations_{:d}.txt'.format(year)\n",
    "    df_year = df[(df[\"date\"] >= pd.Timestamp(year,1,1)) & (df[\"date\"] < pd.Timestamp(year+1, 1, 1))]\n",
    "    quotations = \"\\n\".join(df_year[\"quotation\"].to_list())\n",
    "    print('number of chars {} for year {}'.format(len(quotations), year))\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(quotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total names including aliases of world politicianlen: 340939\n"
     ]
    }
   ],
   "source": [
    "# import politician properties and get their names\n",
    "POLITICIAN = \"../data/filtered_politician_labeled_v3.json.bz2\"\n",
    "pol_df = pd.read_json(POLITICIAN, lines=True, compression='bz2')\n",
    "\n",
    "# extend names with aliases\n",
    "pol_df['aliases_all'] = [a+[b] for a,b in zip(pol_df['aliases'],pol_df['name'])]\n",
    "pol_name_global = pol_df[\"aliases_all\"].tolist()\n",
    "pol_name_global = frozenset([n for names in pol_name_global for n in names])\n",
    "print(\"number of total names including aliases of world politicianlen: {}\".format(len(pol_name_global)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# US\n",
    "# pol_name_us = pol_df[pol_df[\"nationality\"] == \"Q30\"][\"aliases_all\"].tolist()\n",
    "# pol_name_us = frozenset([n for names in pol_name_us for n in names])\n",
    "# pol_name_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import politician properties and get their names\n",
    "# POLITICIAN = \"../data/filtered_politician_labeled_v3.json.bz2\"\n",
    "# pol_df = pd.read_json(POLITICIAN, lines=True, compression='bz2')\n",
    "\n",
    "# # global\n",
    "# process_names = [gensim.utils.simple_preprocess(name) for name in pol_df[\"name\"].tolist()]\n",
    "# global_pol_list = [n for names in process_names for n in names]\n",
    "# global_pol_names = frozenset(global_pol_list)\n",
    "\n",
    "# # US\n",
    "# us_pol_df = pol_df[pol_df[\"nationality\"] == \"Q30\"]\n",
    "# process_names = [gensim.utils.simple_preprocess(name) for name in us_pol_df[\"name\"].tolist()]\n",
    "# # flatten list\n",
    "# us_pol_list = [n for names in process_names for n in names]\n",
    "# us_pol_names = frozenset(us_pol_list)\n",
    "\n",
    "# len(us_pol_names), len(global_pol_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # speed test \n",
    "# %timeit \"zxczxc\" in pol_name_us\n",
    "# %timeit \"zxczxc\" in pol_name_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tagger', <spacy.pipeline.tagger.Tagger at 0x7fd1b71a3640>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize NLP pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# customize pipeline\n",
    "nlp.remove_pipe('lemmatizer') # reduce words to basic form, eg. talking -> talk, president -> presid\n",
    "nlp.remove_pipe('tagger') # tag the part of speech for the token i.e. noun, verb, etc\n",
    "# nlp.remove_pipe('parser') # dependency parser, maybe removed if adding in bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quotations_2015.txt',\n",
       " 'quotations_2016.txt',\n",
       " 'quotations_2017.txt',\n",
       " 'quotations_2018.txt',\n",
       " 'quotations_2019.txt',\n",
       " 'quotations_2020.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "def get_chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "# read quotations as corpus\n",
    "corpus_root = \"../data\"\n",
    "quotations_corpus = PlaintextCorpusReader(corpus_root, \"quotations.*.txt\")\n",
    "quotations_corpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotations_2015.txt\n",
      "Number of sentences: 74375\n",
      "Number of chunks: 744 \n",
      "\n",
      "quotations_2016.txt\n",
      "Number of sentences: 99133\n",
      "Number of chunks: 992 \n",
      "\n",
      "quotations_2017.txt\n",
      "Number of sentences: 146013\n",
      "Number of chunks: 1461 \n",
      "\n",
      "quotations_2018.txt\n",
      "Number of sentences: 143777\n",
      "Number of chunks: 1438 \n",
      "\n",
      "quotations_2019.txt\n",
      "Number of sentences: 119860\n",
      "Number of chunks: 1199 \n",
      "\n",
      "quotations_2020.txt\n",
      "Number of sentences: 37167\n",
      "Number of chunks: 372 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the chunks, otherwise too large to handle\n",
    "corpus_id = {f:n for n,f in enumerate(quotations_corpus.fileids())} # dictionary of books\n",
    "chunks = list()\n",
    "chunk_class = list() # this list contains the original book of the chunk, for evaluation\n",
    "\n",
    "limit = 60 # how many chunks total for one corpus\n",
    "size = 100 # how many sentences per chunk/page\n",
    "\n",
    "for f in quotations_corpus.fileids():\n",
    "    sentences = quotations_corpus.sents(f)\n",
    "    print(f)\n",
    "    print('Number of sentences:',len(sentences))\n",
    "\n",
    "    # create chunks\n",
    "    chunks_of_sents = [x for x in get_chunks(sentences,size)] # this is a list of lists of sentences, which are a list of tokens\n",
    "    chs = list()\n",
    "\n",
    "    # regroup so to have a list of chunks which are strings\n",
    "    for c in chunks_of_sents:\n",
    "        grouped_chunk = list()\n",
    "        for s in c:\n",
    "            grouped_chunk.extend(s)\n",
    "        chs.append(\" \".join(grouped_chunk))\n",
    "    print(\"Number of chunks:\",len(chs),'\\n')\n",
    "\n",
    "    # filter to the limit, to have the same number of chunks per book\n",
    "    chunks.extend(chs[:limit])\n",
    "    chunk_class.extend([corpus_id[f] for _ in range(len(chs[:limit]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# # get processed corpus by nlp pipeline without removing names\n",
    "# processed_docs = []\n",
    "# for doc in nlp.pipe(chunks, n_process=5, batch_size=10):\n",
    "#      # Process document using Spacy NLP pipeline.\n",
    "#     ents = doc.ents  # Named entities\n",
    "\n",
    "#     # Keep only words (no numbers, no punctuation).\n",
    "#     # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "#     # doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "#     # if remove lemmentizer in nlp pipe, what happens here\n",
    "\n",
    "#     # convert token to string while remove punctuation and stopwords\n",
    "#     doc = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "#     # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "#     doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "#     # Add named entities, but only if they are a compound of more than word.\n",
    "#     doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "#     processed_docs.append(doc)\n",
    "\n",
    "# docs = processed_docs\n",
    "# del processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# get processed corpus by nlp pipeline while removing names\n",
    "\n",
    "processed_docs = []\n",
    "for doc in nlp.pipe(chunks, n_process=5, batch_size=10):\n",
    "     # Process document using Spacy NLP pipeline.\n",
    "    ents = doc.ents  # Named entities\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # if remove lemmentizer in nlp pipe, what happens here?\n",
    "    # convert token to string while remove punctuation and stopwords\n",
    "    doc = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "    doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "    # remove names, have to put after adding entities\n",
    "    doc = [token for token in doc if token not in pol_name_global and len(token) > 2]\n",
    "\n",
    "    processed_docs.append(doc)\n",
    "noname_docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if politician names are properly excluded\n",
    "# for chk in noname_docs:\n",
    "#     # print('John Bolton' in noname_docs)\n",
    "#     if 'Donald Trump' in chk:\n",
    "#     # if 'David' in chk:\n",
    "#         print('found')\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Bernie\" in pol_name_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 6528\n",
      "Number of chunks: 500\n"
     ]
    }
   ],
   "source": [
    "# # Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
    "# from gensim.corpora import Dictionary\n",
    "# dictionary = Dictionary(docs)\n",
    "\n",
    "# # Remove rare and common tokens.\n",
    "# # Filter out words that occur too frequently or too rarely.\n",
    "# max_freq = 0.5\n",
    "# min_wordcount = 5\n",
    "# dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# # Bag-of-words representation of the documents.\n",
    "# corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "# #MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "# print('Number of unique tokens: %d' % len(dictionary))\n",
    "# print('Number of chunks: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8033\n",
      "Number of chunks: 360\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(noname_docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.4\n",
    "min_wordcount = 5\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "noname_corpus = [dictionary.doc2bow(doc) for doc in noname_docs]\n",
    "#MmCorpus.serialize(\"models/corpus.mm\", corpus)\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of chunks: %d' % len(noname_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"office\" + 0.002*\"Mitch\" + 0.002*\"party\" + 0.002*\"witnesses\" + 0.002*\"economy\" + 0.002*\"Mitch McConnell\" + 0.002*\"forward\" + 0.002*\"military\" + 0.002*\"continue\" + 0.002*\"let\"'),\n",
       " (1,\n",
       "  '0.002*\"won\" + 0.002*\"coronavirus\" + 0.002*\"Hunter\" + 0.002*\"Ukraine\" + 0.002*\"able\" + 0.002*\"long\" + 0.002*\"continue\" + 0.002*\"Hunter Biden\" + 0.002*\"important\" + 0.002*\"sure\"'),\n",
       " (2,\n",
       "  '0.002*\"coronavirus\" + 0.002*\"world\" + 0.002*\"voters\" + 0.002*\"care\" + 0.002*\"Ukraine\" + 0.002*\"let\" + 0.002*\"job\" + 0.002*\"day\" + 0.002*\"didn\" + 0.002*\"office\"'),\n",
       " (3,\n",
       "  '0.002*\"year\" + 0.002*\"Vice\" + 0.002*\"day\" + 0.002*\"percent\" + 0.002*\"didn\" + 0.002*\"China\" + 0.002*\"voters\" + 0.002*\"won\" + 0.002*\"Ukraine\" + 0.002*\"sure\"')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from gensim.models import LdaMulticore\n",
    "\n",
    "# seed = 1\n",
    "# # random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# params = {'passes': 10, 'random_state': seed}\n",
    "# base_models = dict()\n",
    "# model_2020 = LdaMulticore(corpus=corpus, num_topics=4, id2word=dictionary, workers=6,\n",
    "#                 passes=params['passes'], random_state=params['random_state'])\n",
    "# model_2020.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"Rubio\" + 0.010*\"Marco\" + 0.008*\"Bernie\" + 0.008*\"Cruz\" + 0.007*\"Court\" + 0.007*\"Supreme\" + 0.006*\"Jeb\" + 0.005*\"Ted\" + 0.005*\"South\" + 0.004*\"Carson\"'),\n",
       " (1,\n",
       "  '0.004*\"Attorney\" + 0.004*\"Court\" + 0.004*\"Gorsuch\" + 0.004*\"Steve\" + 0.004*\"marijuana\" + 0.004*\"Supreme\" + 0.003*\"Jeff\" + 0.003*\"Judge\" + 0.003*\"countries\" + 0.003*\"nomination\"'),\n",
       " (2,\n",
       "  '0.003*\"India\" + 0.002*\"Romney\" + 0.002*\"class\" + 0.002*\"financial\" + 0.002*\"education\" + 0.002*\"Paul\" + 0.002*\"ran\" + 0.002*\"Reagan\" + 0.002*\"California\" + 0.002*\"Street\"'),\n",
       " (3,\n",
       "  '0.004*\"racist\" + 0.003*\"Jeff\" + 0.003*\"London\" + 0.003*\"comments\" + 0.003*\"Puerto\" + 0.002*\"Oklahoma\" + 0.002*\"marijuana\" + 0.002*\"billion\" + 0.002*\"sold\" + 0.002*\"program\"'),\n",
       " (4,\n",
       "  '0.005*\"class\" + 0.004*\"middle\" + 0.004*\"budget\" + 0.003*\"taxes\" + 0.003*\"Romney\" + 0.003*\"Henry\" + 0.002*\"Mitt\" + 0.002*\"Jeb\" + 0.002*\"Keystone\" + 0.002*\"tonight\"'),\n",
       " (5,\n",
       "  '0.005*\"Hatch\" + 0.005*\"Romney\" + 0.004*\"Orrin\" + 0.004*\"Utah\" + 0.003*\"probably\" + 0.003*\"goes\" + 0.003*\"news\" + 0.003*\"Mitt\" + 0.003*\"Paul\" + 0.003*\"conference\"'),\n",
       " (6,\n",
       "  '0.004*\"ISIS\" + 0.003*\"King\" + 0.003*\"Pope\" + 0.002*\"strategy\" + 0.002*\"promise\" + 0.002*\"Soleimani\" + 0.002*\"Steve\" + 0.002*\"East\" + 0.002*\"Iranian\" + 0.002*\"happened\"'),\n",
       " (7,\n",
       "  '0.013*\"border\" + 0.011*\"shutdown\" + 0.010*\"wall\" + 0.005*\"Nancy\" + 0.005*\"crisis\" + 0.003*\"Mueller\" + 0.003*\"workers\" + 0.003*\"pay\" + 0.003*\"funding\" + 0.003*\"Speaker\"'),\n",
       " (8,\n",
       "  '0.005*\"trial\" + 0.004*\"impeachment\" + 0.003*\"Warren\" + 0.003*\"witnesses\" + 0.003*\"Russian\" + 0.003*\"Nancy\" + 0.003*\"Flynn\" + 0.003*\"Elizabeth\" + 0.003*\"Bernie\" + 0.002*\"beat\"'),\n",
       " (9,\n",
       "  '0.004*\"Paul\" + 0.002*\"Reagan\" + 0.002*\"Mueller\" + 0.002*\"Saudi\" + 0.002*\"Manafort\" + 0.002*\"investigation\" + 0.002*\"children\" + 0.002*\"pass\" + 0.002*\"happen\" + 0.002*\"terrorists\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seed = 1\n",
    "# random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "params = {'passes': 10, 'random_state': seed}\n",
    "base_models = dict()\n",
    "model_LDA_noname = LdaMulticore(corpus=noname_corpus, num_topics=10, id2word=dictionary, workers=6,\n",
    "                passes=params['passes'], random_state=params['random_state'])\n",
    "\n",
    "model_path = \"../model/lda_bow\"\n",
    "model_LDA_noname.save(model_path)\n",
    "\n",
    "model_LDA_noname.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model and predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rubio', 0.011631411),\n",
       " ('Marco', 0.009772583),\n",
       " ('Bernie', 0.007733384),\n",
       " ('Cruz', 0.007706679),\n",
       " ('Court', 0.0073306244),\n",
       " ('Supreme', 0.0068143285),\n",
       " ('Jeb', 0.0057848063),\n",
       " ('Ted', 0.0052659577),\n",
       " ('South', 0.0045927837),\n",
       " ('Carson', 0.004200234)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '../model/lda_bow'\n",
    "\n",
    "dictionary = Dictionary.load(model_path+'.id2word')\n",
    "lda_model = LdaModel.load(model_path)\n",
    "\n",
    "lda_model.show_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to bow\n",
    "\n",
    "# demo with Trump\n",
    "df_Trump = df[df['qids'] == 'Q22686']\n",
    "df_Trump_before_election = df_Trump[df_Trump['date'] < pd.Timestamp(2017,1,20)]\n",
    "df_Trump_early = df_Trump[(df_Trump['date'] > pd.Timestamp(2017,1,20)) & (df_Trump['date'] < pd.Timestamp(2019,1,20))]\n",
    "df_Trump_late = df_Trump[(df_Trump['date'] > pd.Timestamp(2019,1,20)) & (df_Trump['date'] < pd.Timestamp(2021,1,20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# customize pipeline\n",
    "nlp.remove_pipe('lemmatizer') # reduce words to basic form, eg. talking -> talk, president -> presid\n",
    "nlp.remove_pipe('tagger') # tag the part of speech for the token i.e. noun, verb, etc\n",
    "STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def preprocess_nlp(docs, nlp, n_process=5, batch_size=10):\n",
    "    \"\"\" get processed corpus by nlp pipeline while removing names \"\"\"\n",
    "\n",
    "    processed_docs = []\n",
    "    for doc in nlp.pipe(docs, n_process=n_process, batch_size=batch_size):\n",
    "        # Process document using Spacy NLP pipeline.\n",
    "        ents = doc.ents  # Named entities\n",
    "\n",
    "        # Keep only words (no numbers, no punctuation).\n",
    "        # if remove lemmentizer in nlp pipe, what happens here?\n",
    "        # convert token to string while remove punctuation and stopwords\n",
    "        doc = [token.text for token in doc if token.is_alpha and not token.is_stop]\n",
    "        # print(doc)\n",
    "        # Remove common words from a stopword list and keep only words of length 3 or more.\n",
    "        doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
    "\n",
    "        # Add named entities, but only if they are a compound of more than word.\n",
    "        doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "\n",
    "        # remove names, have to put after adding entities\n",
    "        doc = [token for token in doc if token not in pol_name_global and len(token) > 2]\n",
    "\n",
    "        processed_docs.append(doc)\n",
    "    return processed_docs\n",
    "\n",
    "def domiant_topic(processed_docs, lda_model, dictionary):\n",
    "    bow_vectors = [dictionary.doc2bow(text) for text in processed_docs]\n",
    "    \n",
    "    topic_idx = []\n",
    "    topic_score = []\n",
    "    for row in lda_model[bow_vectors]:\n",
    "        row = sorted(row, key=lambda x: x[1], reverse=True)\n",
    "        topic_idx.append(row[0][0])\n",
    "        topic_score.append(row[0][1])\n",
    "    return topic_idx, topic_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_537/2552915443.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_Trump_before_election['topic_index'] = topic_index\n",
      "/tmp/ipykernel_537/2552915443.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_Trump_before_election['topic_scores'] = topic_scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>phase</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions_qids</th>\n",
       "      <th>urls</th>\n",
       "      <th>topic_index</th>\n",
       "      <th>topic_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42679</th>\n",
       "      <td>2015-01-01-016679</td>\n",
       "      <td>Is Trump Going to Run?</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2015-01-01 13:24:58</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[salon.com]</td>\n",
       "      <td>9</td>\n",
       "      <td>0.366753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45567</th>\n",
       "      <td>2015-01-06-072399</td>\n",
       "      <td>Trump was quoted in a Vanity Fair article abou...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2015-01-06 01:13:40</td>\n",
       "      <td>7</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[watoday.com.au, brisbanetimes.com.au, news.sm...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.617459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22916</th>\n",
       "      <td>2015-01-06-061130</td>\n",
       "      <td>The message pads confiscated from Epstein's ho...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2015-01-06 01:13:40</td>\n",
       "      <td>8</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[watoday.com.au, brisbanetimes.com.au, news.sm...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.702378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19399</th>\n",
       "      <td>2015-01-06-072400</td>\n",
       "      <td>Trump was quoted in a Vanity Fair article abou...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2015-01-06 23:19:21</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Donald Trump]</td>\n",
       "      <td>[Q22686]</td>\n",
       "      <td>[www.brisbanetimes.com.au]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.617741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30140</th>\n",
       "      <td>2015-01-12-085262</td>\n",
       "      <td>Whether [ Obama ] is there or Biden is there o...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Q22686</td>\n",
       "      <td>2015-01-12 14:15:59</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Barack Obama, Joe Biden, John Kerry]</td>\n",
       "      <td>[Q76, Q6279, Q22316]</td>\n",
       "      <td>[insider.foxnews.com]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.549919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 quoteID                                          quotation  \\\n",
       "42679  2015-01-01-016679                             Is Trump Going to Run?   \n",
       "45567  2015-01-06-072399  Trump was quoted in a Vanity Fair article abou...   \n",
       "22916  2015-01-06-061130  The message pads confiscated from Epstein's ho...   \n",
       "19399  2015-01-06-072400  Trump was quoted in a Vanity Fair article abou...   \n",
       "30140  2015-01-12-085262  Whether [ Obama ] is there or Biden is there o...   \n",
       "\n",
       "            speaker    qids                date  numOccurrences phase  \\\n",
       "42679  Donald Trump  Q22686 2015-01-01 13:24:58               1     E   \n",
       "45567  Donald Trump  Q22686 2015-01-06 01:13:40               7     E   \n",
       "22916  Donald Trump  Q22686 2015-01-06 01:13:40               8     E   \n",
       "19399  Donald Trump  Q22686 2015-01-06 23:19:21               1     E   \n",
       "30140  Donald Trump  Q22686 2015-01-12 14:15:59               1     E   \n",
       "\n",
       "                                    mentions         mentions_qids  \\\n",
       "42679                         [Donald Trump]              [Q22686]   \n",
       "45567                         [Donald Trump]              [Q22686]   \n",
       "22916                         [Donald Trump]              [Q22686]   \n",
       "19399                         [Donald Trump]              [Q22686]   \n",
       "30140  [Barack Obama, Joe Biden, John Kerry]  [Q76, Q6279, Q22316]   \n",
       "\n",
       "                                                    urls  topic_index  \\\n",
       "42679                                        [salon.com]            9   \n",
       "45567  [watoday.com.au, brisbanetimes.com.au, news.sm...            8   \n",
       "22916  [watoday.com.au, brisbanetimes.com.au, news.sm...            8   \n",
       "19399                         [www.brisbanetimes.com.au]            8   \n",
       "30140                              [insider.foxnews.com]            0   \n",
       "\n",
       "       topic_scores  \n",
       "42679      0.366753  \n",
       "45567      0.617459  \n",
       "22916      0.702378  \n",
       "19399      0.617741  \n",
       "30140      0.549919  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_Trump_before_election))\n",
    "\n",
    "docs = df_Trump_before_election[\"quotation\"].to_list()\n",
    "# print('example: ', docs)\n",
    "\n",
    "processed_docs = preprocess_nlp(docs)\n",
    "# print('after processing: ', processed_docs)\n",
    "\n",
    "topic_index, topic_scores = domiant_topic(processed_docs, lda_model, dictionary)\n",
    "\n",
    "df_Trump_before_election['topic_index'] = topic_index\n",
    "df_Trump_before_election['topic_scores'] = topic_scores\n",
    "df_Trump_before_election.head()\n",
    "\n",
    "\n",
    "# corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "# processed_docs\n",
    "# topic_distribution(processed_docs, lda_model, dictionary)\n",
    "# processed_docs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='topic_index', ylabel='Count'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn  as sns\n",
    "sns.histplot(df_Trump_before_election['topic_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Vitter pursues similar line of attack to Bobby Jindal on Common Core Chuck Kleckley is considering a run for Louisiana State Treasurer\n",
      "-0.3818\n",
      "David Vitter pursues similar line of attack to Bobby Jindal on Common Core Steve Scalise should use House whip position as `bully pulpit' to address racism, Jewish anti-hate group says\n",
      "-0.802\n",
      "I've joked around a little bit with Rahm Emanuel, and there's a chance we share parts of it,\n",
      "0.6705\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#  for now analyzer is using built-in general purpose lexicon\n",
    "# can be trained on our own data\n",
    "sentences = df['quotation'].iloc[:3]\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    print(sid.polarity_scores(s)['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, bz2, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 525.40]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 544.16]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 320.02]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 520.70]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 541.09]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 528.17]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 534.60]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 553.37]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 491.75]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 514.69]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 564.19]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 581.06]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 553.18]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 587.39]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 608.92]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 588.13]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 564.96]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 575.14]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 592.63]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 572.95]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 578.49]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 577.78]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 582.36]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 562.91]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 577.50]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 545.99]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 558.51]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 552.40]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 581.46]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 553.77]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 567.50]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 543.77]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 537.09]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 579.99]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 575.94]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 577.24]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 343.81]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 522.99]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 556.01]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 497.62]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 552.76]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 540.21]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 528.60]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 548.33]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 551.35]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 557.23]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 566.98]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 563.55]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 538.53]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 548.24]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 524.60]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 587.63]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 570.69]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 546.64]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 558.28]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 572.62]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 549.35]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 520.46]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 495.87]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 546.42]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 566.70]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 562.32]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 562.25]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 527.59]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 562.52]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 578.75]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 599.01]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 602.30]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 594.39]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 600.32]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 588.46]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 586.70]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 588.37]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 582.31]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 596.21]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 594.43]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 585.39]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 567.26]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 552.07]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 551.44]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 546.94]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 571.79]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 575.77]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 561.72]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 576.72]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 588.89]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 597.65]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 565.63]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 587.41]\n",
      "Dumped 5000 quotations out of 5000 quotations [quotations/s: 596.17]\n",
      "Dumped 436 quotations out of 5000 quotations [quotations/s: 5548.46]\n"
     ]
    }
   ],
   "source": [
    "SAVE_FILE = '../data/quotes_mentions_USA_ML.json.bz2'\n",
    "READ_FILE = '../data/quotes_mentions_USA_compact.json.bz2'\n",
    "\n",
    "CHUNKSIZE=5000\n",
    "\n",
    "try:\n",
    "    os.remove(SAVE_FILE)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "total_time,total_mentions,chunk_number,nb_quote=0,0,0,0\n",
    "mentions=[]\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "with bz2.open(SAVE_FILE, 'wb') as bz_writer:\n",
    "    with pd.read_json(READ_FILE, lines=True, chunksize=CHUNKSIZE, compression='bz2' ) as df_reader:\n",
    "        for chunk in df_reader:\n",
    "            t1=time.time()\n",
    "\n",
    "            # LDA topic\n",
    "            docs = chunk[\"quotation\"].to_list()\n",
    "            processed_docs = preprocess_nlp(docs, nlp)\n",
    "            topic_index, topic_scores = domiant_topic(processed_docs, lda_model, dictionary)\n",
    "            chunk['topic_index'] = topic_index\n",
    "            chunk['topic_scores'] = topic_scores\n",
    "\n",
    "            # sentiment analysis\n",
    "            sentiment_score = [sid.polarity_scores(s)['compound'] for s in docs]\n",
    "            chunk['sentiment'] = sentiment_score\n",
    "\n",
    "            chunk.to_json(path_or_buf=bz_writer,orient='records',lines=True)\n",
    "            mentions = len(chunk)\n",
    "            total_mentions += mentions\n",
    "            t2=time.time()\n",
    "            dt=t2-t1\n",
    "            total_time+=dt\n",
    "            chunk_number += 1\n",
    "            print(\"Dumped {} quotations out of {} quotations [quotations/s: {:.2f}]\".format(mentions, CHUNKSIZE, CHUNKSIZE / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>phase</th>\n",
       "      <th>mentions</th>\n",
       "      <th>mentions_qids</th>\n",
       "      <th>urls</th>\n",
       "      <th>topic_index</th>\n",
       "      <th>topic_scores</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-25-000242</td>\n",
       "      <td>' It is not now, nor has it ever been, the gol...</td>\n",
       "      <td>Bernie Sanders</td>\n",
       "      <td>Q359442</td>\n",
       "      <td>2015-10-25 14:12:35</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Bill Clinton]</td>\n",
       "      <td>[Q1124]</td>\n",
       "      <td>[examiner.com]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.818588</td>\n",
       "      <td>-0.4822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-07-005048</td>\n",
       "      <td>All we â•’ re asking for here is a discussion an...</td>\n",
       "      <td>John Boehner</td>\n",
       "      <td>Q11702</td>\n",
       "      <td>2015-08-07 12:52:52</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[Barack Obama]</td>\n",
       "      <td>[Q76]</td>\n",
       "      <td>[liveblog.irishtimes.com]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.774811</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-01-005722</td>\n",
       "      <td>An email included in the latest tranche of Cli...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>Q6294</td>\n",
       "      <td>2015-10-01 14:56:48</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>[Bill Clinton]</td>\n",
       "      <td>[Q1124]</td>\n",
       "      <td>[feeds.foxnews.com, www.foxnews.com]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.368826</td>\n",
       "      <td>-0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-11-17-006368</td>\n",
       "      <td>and in fact, Secretary of State Kerry was earl...</td>\n",
       "      <td>Phil Bryant</td>\n",
       "      <td>Q887898</td>\n",
       "      <td>2015-11-17 20:03:05</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>[John Kerry]</td>\n",
       "      <td>[Q22316]</td>\n",
       "      <td>[hottytoddy.com]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774721</td>\n",
       "      <td>0.0258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-14-014011</td>\n",
       "      <td>I have fought Obamacare from Day One and will ...</td>\n",
       "      <td>John Cornyn</td>\n",
       "      <td>Q719568</td>\n",
       "      <td>2015-02-14 21:01:51</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>[Barack Obama]</td>\n",
       "      <td>[Q76]</td>\n",
       "      <td>[www.politico.com, politico.com]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389230</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-10-25-000242  ' It is not now, nor has it ever been, the gol...   \n",
       "1  2015-08-07-005048  All we â•’ re asking for here is a discussion an...   \n",
       "2  2015-10-01-005722  An email included in the latest tranche of Cli...   \n",
       "3  2015-11-17-006368  and in fact, Secretary of State Kerry was earl...   \n",
       "4  2015-02-14-014011  I have fought Obamacare from Day One and will ...   \n",
       "\n",
       "           speaker     qids                date  numOccurrences phase  \\\n",
       "0   Bernie Sanders  Q359442 2015-10-25 14:12:35               1     E   \n",
       "1     John Boehner   Q11702 2015-08-07 12:52:52               1     E   \n",
       "2  Hillary Clinton    Q6294 2015-10-01 14:56:48               2     E   \n",
       "3      Phil Bryant  Q887898 2015-11-17 20:03:05               1     E   \n",
       "4      John Cornyn  Q719568 2015-02-14 21:01:51               2     E   \n",
       "\n",
       "         mentions mentions_qids                                  urls  \\\n",
       "0  [Bill Clinton]       [Q1124]                        [examiner.com]   \n",
       "1  [Barack Obama]         [Q76]             [liveblog.irishtimes.com]   \n",
       "2  [Bill Clinton]       [Q1124]  [feeds.foxnews.com, www.foxnews.com]   \n",
       "3    [John Kerry]      [Q22316]                      [hottytoddy.com]   \n",
       "4  [Barack Obama]         [Q76]      [www.politico.com, politico.com]   \n",
       "\n",
       "   topic_index  topic_scores  sentiment  \n",
       "0            8      0.818588    -0.4822  \n",
       "1            8      0.774811     0.0000  \n",
       "2            4      0.368826    -0.2732  \n",
       "3            1      0.774721     0.0258  \n",
       "4            2      0.389230    -0.1280  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_FILE = '../data/quotes_mentions_USA_ML.json.bz2'\n",
    "df = pd.read_json(SAVE_FILE, lines=True, compression='bz2' )\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfModel(corpus=noname_docs, id2word=dictionary)\n",
    "# tfidf_corpus = tfidf[noname_docs] \n",
    "\n",
    "# params = {'passes': 10, 'random_state': seed}\n",
    "# base_models = dict()\n",
    "# model_2020_noname_tfidf = LdaMulticore(corpus=tfidf_corpus, num_topics=4, id2word=dictionary, workers=6,\n",
    "#                 passes=params['passes'], random_state=params['random_state'])\n",
    "# model_2020_noname_tfidf.show_topics(num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass in name list to remove politician names\n",
    "# sample_quote = df_2020['quotation'].iloc[0]\n",
    "# print(\"Sample quotation: \", sample_quote)\n",
    "# preprocess(sample_quote, us_pol_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # process 2020 data\n",
    "# processed_docs = []\n",
    "# for quotation in df_2020['quotation'].tolist():\n",
    "#     processed_docs.append(preprocess(quotation, pol_name_us))\n",
    "# #words into corpus\n",
    "# dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "# #convert to bag of words corpus\n",
    "# bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfModel(bow_corpus, id2word=dictionary)\n",
    "# tfidf_corpus = tfidf[bow_corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train LDA model with bow corpus\n",
    "# num_topics = 5\n",
    "# lda_2020_bow =  gensim.models.LdaMulticore(bow_corpus, \n",
    "#                                    num_topics = num_topics, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 10)\n",
    "# MODEL_PATH = \"./lda_bow_2020\"\n",
    "# lda_2020_bow.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyze model\n",
    "# for idx, topic in lda_2020_bow.print_topics(-1):\n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train LDA model with tfidf corpus\n",
    "# num_topics = 5\n",
    "# lda_2020_tfidf =  gensim.models.LdaMulticore(tfidf_corpus, \n",
    "#                                    num_topics = num_topics, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 10)\n",
    "# MODEL_PATH = \"./lda_tfidf_2020\"\n",
    "# lda_2020_tfidf.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # analyze model\n",
    "# for idx, topic in lda_2020_tfidf.print_topics(-1):\n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use the model to predict\n",
    "# test_document = df_2020['quotation'].iloc[0]\n",
    "# test_processed = preprocess(test_document, us_pol_names)\n",
    "# bow_vector = dictionary.doc2bow(test_processed)\n",
    "\n",
    "# # bow model\n",
    "# print('original quotation: \"{}\"'.format(test_document))\n",
    "# print('processed input: \"{}\"'.format(test_processed))\n",
    "# for index, score in sorted(lda_2020_bow[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "#     print(\"Score: {}\\t Topic: {}\".format(score, lda_2020_bow.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfidf model \n",
    "# print('original quotation: \"{}\"'.format(test_document))\n",
    "# print('processed input: \"{}\"'.format(test_processed))\n",
    "# for index, score in sorted(lda_2020_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "#     print(\"Score: {}\\t Topic: {}\".format(score, lda_2020_tfidf.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a4b9e7808f4744fd74a39db385be6b30c2ca4cc57deaa44443ca6af78ad974"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyenv_ada': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
